{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Continuous Bag of Words Model",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOFwte3eMPhZB17wEZOvjtp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psych0man/Continuous-Bag-of-Words-Model/blob/master/Continuous_Bag_of_Words_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLPKDKeMebL",
        "colab_type": "text"
      },
      "source": [
        "# Importing Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmKaqR8XLvnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUw-mCrBMrZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.data.path.append('.')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZASoTuWNeMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKjtn920M0Ic",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6kDY7DSMyNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "with open('shakespeare.txt') as f:\n",
        "  data = f.read()\n",
        "data = re.sub(r'[,!?;-]', '.',data) # Punctuations\n",
        "data = nltk.word_tokenize(data)\n",
        "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.'] # Lowercase and non-alphabetical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp4AGy4YNZqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "freq_dist = nltk.FreqDist(word for word in data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXPxsdsIN1NK",
        "colab_type": "text"
      },
      "source": [
        "# Training the CBOW Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNhcY931Nr2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize(N,V, random_seed=1):\n",
        "  np.random.seed(random_seed)\n",
        "\n",
        "  w1 = np.random.rand(N,V)\n",
        "  w2 = np.random.rand(V,N)\n",
        "  b1 = np.random.rand(N,1)\n",
        "  b2 = np.random.rand(V,1)\n",
        "\n",
        "  return w1,w2,b1,b2"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEnVmMwZOSc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(z):\n",
        "  e = np.exp(z)\n",
        "  y_hat = e/e.sum(e,axis=0)\n",
        "\n",
        "  return y_hat"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyK6zw1kOhXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation(x,w1,w2,b1,b2):\n",
        "\n",
        "  h = np.dot(w1,x) + b1\n",
        "  h = np.maximum(0,h) # Make-shift ReLU\n",
        "  z = np.dot(w2,h)+b2\n",
        "\n",
        "  return z,h"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ8dENNvO62E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cost(y,y_hat, batch_size):\n",
        "  log_prob = np.multiply(np.log(y_hat),y) + np.multiply(np.log(1 - y_hat), 1 - y)\n",
        "  cost = -1/batch_size * np.sum(log_prob)\n",
        "  cost = np.squeeze(cost)\n",
        "\n",
        "  return cost"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0blsif3PUyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def back_prob(x, y_hat, y, h, w1, w2, b1, b2, batch_size):\n",
        "  l1 = np.dot(w2.T,(y_hat-y))\n",
        "  l1 = np.maximum(0,l1) # Make-shift ReLU\n",
        "  grad_w1 = (1/batch_size)*np.dot(l1,x.T)\n",
        "  grad_w2 = (1/batch_size)*np.dot(y_hat-y,h.T)\n",
        "  grad_b1 = np.sum((1/batch_size)*np.dot(l1,x.T),axis=1,keepdims=True)\n",
        "  grad_b2 = np.sum((1/batch_size)*np.dot(y_hat-y,h.T),axis=1,keepdims=True)\n",
        "\n",
        "  return grad_w1, grad_w2, grad_b1, grad_b2"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw6_33xHQHWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(data, word2Ind, V, C, batch_size):\n",
        "    batch_x = []\n",
        "    batch_y = []\n",
        "    for x, y in get_vectors(data, word2Ind, V, C):\n",
        "        while len(batch_x) < batch_size:\n",
        "            batch_x.append(x)\n",
        "            batch_y.append(y)\n",
        "        else:\n",
        "            yield np.array(batch_x).T, np.array(batch_y).T\n",
        "            batch = []"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_eIccn_PttA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad_descent(data, word2Ind, N, V, num_iters, alpha=0.03):\n",
        "\n",
        "    w1, w2, b1, b2 = initialise(N,V, random_seed=282)\n",
        "    batch_size = 128\n",
        "    iters = 0\n",
        "    C = 2\n",
        "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
        "\n",
        "        z, h = forward_prop(x, w1, w2, b1, b2)\n",
        "        yhat = softmax(z)\n",
        "        cost = cost(y, y_hat, batch_size)\n",
        "\n",
        "        if ( (iters+1) % 10 == 0):\n",
        "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
        "        \n",
        "        grad_w1, grad_w2, grad_b1, grad_b2 = back_prop(x, y_hat, y, h, w1, w2, b1, b2, batch_size)\n",
        "        \n",
        "        w1 -= alpha*grad_w1 \n",
        "        w2 -= alpha*grad_w2\n",
        "        b1 -= alpha*grad_b1\n",
        "        b2 -= alpha*grad_b2\n",
        "        \n",
        "        \n",
        "        iters += 1 \n",
        "        if iters == num_iters: \n",
        "            break\n",
        "        if iters % 100 == 0:\n",
        "            alpha *= 0.66\n",
        "            \n",
        "    return w1, w2, b1, b2"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}